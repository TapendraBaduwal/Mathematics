{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyNYSBVpPYFNZ+b+uGeRhqzO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Calculus for Machine Learning**"],"metadata":{"id":"Rk25tIi_ADFw"}},{"cell_type":"markdown","source":["\n","#**Partial derivatives**\n","\n","###**Ordinary Least Squares(OLS) with Simple Linear Regression:**\n","\n","Lets we have actual output value of y is  $y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$\n","\n","Also we have predicted  output  value of y is  $\\hat{y_i} = \\beta_0 + \\beta_1x_i$\n","\n","where $\\epsilon_i$ is error and $x_i$ is positional independent input variables.\n","\n","To calculate  the value of $\\beta_{0}$ and $\\beta_{1}$ with best-fit regression line, we minimize the Residual Sum of Squares(RSS) also called Sum of Squared Errors (SSE). \n","\n","Sum of Squared Errors (SSE) can also be written as:\n","\n"," $$\\text{SSE} = \\sum_{i=1}^{n}(y_{i}-\\hat{y_{i}})^2 =\\sum_{i=1}^{n}(y_{i}-(\\beta_0+\\beta_1x_i))^2 $$\n","\n","  In above we replaced simple linear regression  equation $\\hat{y_i} = \\beta_0 + \\beta_1x_i$\n","\n","**Lets Calculate $\\beta_0$ :**\n","\n","Firstly taking Partial derivatives(Partial derivatives means doing derivatives with respect of single variables and other variable remain constant) with respect to $\\beta_0$:\n","\n","$$\\frac{\\partial\\ \\text{SSE}}{\\partial \\beta_0}  = \\frac{\\partial }{\\partial \\beta_0}\\sum(y_i-(\\beta_0+\\beta_1x_i))^2$$\n","\n","\n","Now lets apply  chain rule and power rule(apply chain rule for making base of derivatives same) then, we get:\n","\n","\n","$$=\\sum\\frac{\\partial }{\\partial (y_i-(\\beta_0+\\beta_1x_i)}(y_i-(\\beta_0+\\beta_1x_i))^2 .\\frac{\\partial }{\\partial \\beta_0}(y_i-(\\beta_0+\\beta_1x_i))$$\n","\n","After  applying chain rule and power rule we get:\n","\n"," $$=2\\sum(y_i-(\\beta_0+\\beta_1x_i))(\\frac{\\partial\\ \\text{$y_i$}}{\\partial \\beta_0}-\\frac{\\partial\\ \\text{$\\beta_0$}}{\\partial \\beta_0}-\\frac{\\partial\\ \\text{$\\beta_1$$x_i$}}{\\partial \\beta_0})$$\n","\n","\n","$$=2\\sum(y_i-(\\beta_0+\\beta_1x_i))(0-1-0)$$\n","\n","$$=-2\\sum(y_i-(\\beta_0+\\beta_1x_i))\\tag{i}$$\n","\n","\n","Since,our main aim is to reduce error towards zero($0$) so,lets set up the partial derivatives equal to $0$ for equation $(i)$.\n","\n","$$-2\\sum(y_i-(\\beta_0+\\beta_1x_i))  = 0\\tag{ii}$$\n","\n","\n","Now we are going to solve the above equation to find our required parameters($\\beta_0$)\n"," \n","\n"," **Solving equation (ii) for $\\beta_0$**\n","\n"," lets divide equation (ii) in both side by $-2$ then we get,\n","\n","$$\\sum(y_i-(\\beta_0+\\beta_1x_i))  = 0$$\n","\n","Apply  summation in each terms inside the bracket, then we get:\n","\n","$$\\sum y_i- \\sum \\beta_0 - \\sum \\beta_1x_i = 0$$\n","\n","\n","Since, $\\beta_0 $ and $\\beta_1$ are random variables and take  any random value. But the values they take are constant over the samples.With respect to summation over the samples dataset,$\\beta_0 $ and $\\beta_1$  are constants so they can come outside the summation as follows:\n","\n","$$\\sum y_i- \\beta_0 - \\beta_1\\sum x_i = 0$$\n","\n","\n","Since,$\\beta_0$ written as $n\\beta_0$  because  from $1$ to $n$  data points the sum of each intercept values of data points is $n$ times of $\\beta_0$  then, we write;\n","\n","$$n\\beta_0 = \\sum y_i- \\beta_1\\sum x_i$$\n","\n","lets dividing both sides by $n$,then  we get:\n","\n","$$\\beta_0 = \\frac{\\sum y_i}{n}- \\frac{\\beta_1\\sum x_i}{n}$$\n","\n","Since the sum of all values of $y's$ divided by $n$ gives the mean or average value and also the sum of all values of $x's$ divided by $n$ gives the mean or average value.\n","\n","Finally we get the value of $\\beta_0$ as follows:\n","\n","\n","$$\\beta_0 = \\overline{y}- \\beta_1\\overline{x}\\tag{iii}$$\n","\n","\n","**Similarly lets calculate $\\beta_1$**\n","\n","Again taking Partial derivatives with respect to $\\beta_1$\n","\n","$$\\frac{\\partial\\ {\\text{SSE}} }{\\partial \\beta_1} = \\frac{\\partial }{\\partial \\beta_1}\\sum(y_i-(\\beta_0+\\beta_1x_i))^2$$\n","\n","Now,Put the value of $\\beta_0$ from above equation(iii) then, we get:\n","\n","\n","$$ = \\frac{\\partial }{\\partial \\beta_1}\\sum(y_i-(\\overline{y}- \\beta_1\\overline{x}+\\beta_1x_i))^2$$\n","\n","\n","Now lets apply  chain rule and power rule(apply chain rule for making base of derivatives same) then, we get:\n","\n","$$ ={\\sum\\frac{\\partial }{\\partial ((y_i-(\\overline{y}- \\beta_1\\overline{x}+\\beta_1x_i))}(y_i-(\\overline{y}- \\beta_1\\overline{x}+\\beta_1x_i))^2.\\frac{\\partial }{\\partial \\beta_1}(y_i-(\\overline{y}- \\beta_1\\overline{x}+\\beta_1x_i))}$$\n","\n","After  applying chain rule and power rule we get:\n","\n"," $$=2\\sum(y_i-(\\overline{y}- \\beta_1\\overline{x}+\\beta_1x_i)).(\\frac{\\partial\\ \\text{$y_i$}}{\\partial \\beta_1}-\\frac{\\partial\\ \\text{$\\overline{y}$}}{\\partial \\beta_1}+\\frac{\\partial\\ \\text{$\\beta_1$$\\overline{x}$}}{\\partial \\beta_1}-\\frac{\\partial\\ \\text{$\\beta_1$$x_i$}}{\\partial \\beta_1})$$\n","\n","\n","$$=2\\sum[(y_i-\\overline{y})- \\beta_1(x_i-\\overline{x})].(0-0+\\overline{x}-x_i)$$\n","\n","$$=2\\sum[(y_i-\\overline{y})- \\beta_1(x_i-\\overline{x})].[-(x_i-\\overline{x})]$$\n","\n"," $$=-2\\sum[(y_i-\\overline{y})- \\beta_1(x_i-\\overline{x})].[(x_i-\\overline{x})]\\tag{iv}$$\n","\n"," Since,our main aim is to reduce error towards zero($0$) so,lets set up the partial derivatives equal to $0$ for equation (iv).\n","\n","$$-2\\sum[(y_i-\\overline{y})- \\beta_1(x_i-\\overline{x})].[(x_i-\\overline{x})]=0\\tag{v}$$\n","\n","lets divide equation (v) in both side by $-2$ then we get,\n","\n","$$\\sum[(y_i-\\overline{y})- \\beta_1(x_i-\\overline{x})].[(x_i-\\overline{x})]=0$$\n","\n","Lets multiply by last outside bracket value with inside other values then we get;\n","\n","$$\\sum[(y_i-\\overline{y})(x_i-\\overline{x})- \\beta_1(x_i-\\overline{x})(x_i-\\overline{x})]=0$$\n","\n","$$\\sum(y_i-\\overline{y})(x_i-\\overline{x})- \\beta_1\\sum(x_i-\\overline{x})^2=0$$\n","\n","$$\\beta_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\\tag{vi}$$\n","\n","**Finally we get Best fitting Parameters $\\beta_0$ and $\\beta_1$:**\n","\n","In general we put **hats** symboll on our $\\beta_0$ and $\\beta_0$ parameters because parameters are estimates.\n","\n","$$\\hat{\\beta_0} = \\bar{y} - \\beta_1\\bar{x}$$\n","\n","\n","$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n","\n"],"metadata":{"id":"KX99GPX2BHgF"}},{"cell_type":"markdown","source":["###**Limitations of  Ordinary Least Squares(OLS):**\n","- In **OLS with Multiple Linear Regression**(use same calculation process like OLS with simple linear regression) we obtain the  parameters as:\n","$$\\hat{\\boldsymbol{\\beta}} =(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}$$\n","\n","-  Here, $\\mathbf{X}$ is in the form of matrix form.\n","- From above if  matrix is not a full rank matrix then  **inverse of matrix doen not exist**  in this case OLS is not suitables.\n","\n","- OLS requires the number of samples(n ) must be greater than the number of features(d).\n","- To overcome this proablems  we used **Gradient Descent.**\n","\n"],"metadata":{"id":"7yq2YF_rBk3w"}},{"cell_type":"markdown","source":["##**Gradient Descent**\n","\n","- Gradient Descent is an algorithm to minimize the **cost function(error function)**  by **optimizing(Best fitting) its parameters.**\n","\n","- The cost function($J$) is  the **sum of squared error multiplied by $\\frac{1}{2}$ to make the derivation easier.**\n","\n","- To find the **best-fit regression line**, the model aims to predict **$y$** value such that the error difference between Actual value and predicted value is minimum.\n","- So, it is very important to update the value of  $β_0$, $β_1$,$β_2$,....,$β_n$ , to reach the best value that **minimize the error** between Actual output value and  predicted output value.\n","\n","- It is done by a random selection of values of coefficient and then iteratively update the values to reach the minimum cost function.\n","\n","- Gradient Descent  is mostly used in Linear regression, logistical regression,Principal component analysis (PCA),neural network etc and other various  machine learning and deep learning.\n","\n","\n","<figure align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1ln_I8G7cq79Lzbn5dy4djr8jbeyrgdCq\" height=\"300px\", width=\"500px\"> \n","</figure>\n","\n","####**Lets  we have initial values of Paremeters $β_0$, $β_1$,$β_2$,....,$β_n$ (which is Randomly Initialization)**\n","\n","\n","$$\\boldsymbol{\\beta} =\\begin{bmatrix}\n","\\beta_0 \\\\ \n","\\beta_1 \\\\\n","\\beta_2 \\\\\n","\\vdots \\\\\n","\\beta_n\n","\\end{bmatrix}$$\n","\n","####**Since we have a cost function as below:**\n","\n","Lets we have actual output value of y is  $y_i = \\beta_0+\\beta_1x_{i1} +\\beta_2x_{i2} +....+ \\beta_nx_{in} + \\epsilon_i$\n","\n","Also we have predicted  output  value of y is  $\\hat{y_i} = \\beta_0+\\beta_1x_{i1} +\\beta_2x_{i2} +....+ \\beta_nx_{in}$\n","\n","where $\\epsilon_i$ is error and $x_i$ is positional independent input variables.\n","\n","Then the Cost Function is written as:\n","\n","\n","\\begin{align}\n","J(\\beta_0, \\beta_1, \\beta_2,..., \\beta_n) &= \\frac{1}{2}\\sum_{i=1}^{n}({y_{i}}-\\hat{y_{i}})^2 \\\\ \n","&= \\frac{1}{2}\\sum_{i=1}^{n}(y_{i}-(\\beta_0+\\beta_1x_{i1} +\\beta_2x_{i2} +....+ \\beta_nx_{in}))^2\n","\\end{align}\n","\n","###**Taking Partial derivative of the cost function with respect to  $\\beta_0$ then we get:**\n","\n","$$\\frac{\\partial\\ \\text{J}}{\\partial \\beta_0}  = \\frac{1}{2}\\sum_{i=1}^{n}\\frac{\\partial }{\\partial \\beta_0}(y_{i}-\\hat{y_{i}})^2$$\n","\n","Now apply chain and power rules:\n","$$=\\frac{1}{2}\\ \\sum_{i=1}^{n}\\frac{\\partial ({y_{i}}-\\hat{y_{i}})^2}\n","{\\partial ({y_{i}}-\\hat{y_{i}})} \\times \\frac{\\partial ({y_{i}}-\\hat{y_{i}})}{\\partial \\beta_0}$$\n","\n","\n","$$=\\sum_{i=1}^{n}({y_i} - \\hat{y_i}) \\times \\frac{\\partial (y_i-(\\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} +...+ \\beta_nx_{in}))}{\\partial \\beta_0}$$\n","\n","$$=\\sum_{i=1}^{n}({y_i} - \\hat{y_i}) \\times (-1)$$\n","\n","$$=-\\sum_{i=1}^{n}({y_i} - \\hat{y_i})$$\n","\n","\n","Similarly ,Taking Partial derivative of the cost function with respect to  $\\beta_1$,$\\beta_2$,...,$\\beta_n$ then we get:\n","\n","$$\\frac{\\partial\\ \\text{J}}{\\partial \\beta_1} =-\\sum_{i=1}^{n}({y_i} - \\hat{y_i})x_{i1}$$\n","\n","$$\\frac{\\partial\\ \\text{J}}{\\partial \\beta_2} =-\\sum_{i=1}^{n}({y_i} - \\hat{y_i})x_{i2}$$\n","\\begin{align}\\dots \\\\\\dots \\\\\\end{align}\n","\n","$$\\frac{\\partial\\ \\text{J}}{\\partial \\beta_n} =-\\sum_{i=1}^{n}({y_i} - \\hat{y_i})x_{in}$$\n","\n","Finally we  can write in the matrix form as below:\n","$$\\frac{\\boldsymbol{\\partial J}}{\\boldsymbol{\\partial \\beta}} = \\begin{bmatrix}\n","\\frac{\\partial J}{\\partial \\beta_0} \\\\ \n","\\frac{\\partial J}{\\partial \\beta_1}\\\\\n","\\frac{\\partial J}{\\partial \\beta_2}\\\\\n","\\vdots \\\\\n","\\frac{\\partial J}{\\partial \\beta_n}\n","\\end{bmatrix} $$\n","\n","\n","###**Lets update the value of parameters(optimizing its parameters to get Best fitting regression line) until the cost function converges to its minimum value:**\n","\n","Repeat the process until cost function converges to its minimum value:\n","$$\\beta_0 :=\\beta_0 - \\alpha\\frac{\\partial J}{\\partial \\beta_0}$$\n","\n","$$\\beta_1 :=\\beta_1-\\alpha\\frac{\\partial J}{\\partial \\beta_1}$$\n","\n","$$\\beta_2 :=\\beta_2-\\alpha\\frac{\\partial J}{\\partial \\beta_2}$$\n","\n","$$\\vdots \\\\\n","\\beta_n :=\\beta_n-\\alpha\\frac{\\partial J}{\\partial \\beta_n}$$\n","\n","**Note:** The  slop may be +ve or -ve,the slop of line decides in which direction to moves.That help to decide whether we have to add or subtract  on old paremeters to get new paremeters.New gradient tells us the slope of our cost function at our current position and the direction we should move to update our parameters.\n","\n","\n","###**LEARNING RATE:**\n","Here, $\\alpha$ is learning rate which determine the size of the step that we take.\n","This parameter determines how fast or slow we move towards the optimal weights while minimizing cost function at every step. High learning rate could cover more area per step but would risk overshooting the minima; low learning rate would take virtually forever to reach the minma.\n"],"metadata":{"id":"HuusQiMvBpMO"}}]}