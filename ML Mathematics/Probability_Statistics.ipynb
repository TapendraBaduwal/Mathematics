{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Probability_Statistics.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMkvnY822hxWPzOoeHgCImo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Probability for Machine Learning** "],"metadata":{"id":"oTzF9_4d6zU0"}},{"cell_type":"markdown","source":["#**Bayes' Theorem**\n","\n","- Bayes Theorem is a method to determine conditional probabilities that is, **the probability of one event occurring given that another event has already occurred.**\n","\n","- The formula for Bayes' theorem is given as:\n","$$P(\\text{A}|\\text{B})=\\frac{P(\\text{B}|\\text{A}) \\ P(\\text{A})}{P(\\text{B})}$$\n","\n","- Where,$P(\\text{A}|\\text{B})$ is Probability of A occuring given evidence B has already occured( P(A|B) is also called posterior probability),$P(\\text{B}|\\text{A})$ is Probability of B occuring given evidence A has already occured(P(B|A) is  also called Likelihood probability),$P(\\text{A})$ is Probability of A occuring,$P(\\text{B})$ is Probability of B occuring.\n","\n","\n","- Lets $y$ is dependent(target) variable and ($x_1$, $x_2$, $x_3$,....., $x_n$) are independent variables features.Then we can write above formula as follows also,\n","\n","$$P(y|x_1, x_2 ..... x_n) = \\frac{P(x_1, x_2 ... x_i ... x_n|y) \\ P(y)}{P(x_1, x_2 ... x_i ... x_n)}$$\n","\n","$$P(y|x_1, x_2 ..... x_n) = \\frac{[P(x_1|y)P(x_2|y)......P(x_n|y)] \\ P(y)}{P(x_1)P(x_2)..... P(x_n)}$$\n","\n","\n","\n","##**Working of Naive Bayes' Classifier:**\n","\n","- Suppose we have a dataset of weather conditions and corresponding target variable \"Play\". So using this dataset we need to decide that whether we should play or not on a particular day according to the weather conditions\n","\n","- **step1:**Convert the given dataset into frequency tables.\n","- **step2:**Generate Likelihood table by finding the probabilities of given features.\n","- **step3:**Now, use Bayes theorem to calculate the posterior probability.\n","\n","\n","\n","<figure align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1T4wUb-EjKhD6ceMQPwJgfiuD5VW1nGnr\" height=\"400px\", width=\"600px\"> \n","</figure>\n","\n","- **Applying Bayes'theorem:**\n","\n","- P(Yes|Sunny)= P(Sunny|Yes)*P(Yes)/P(Sunny)\n","\n","- P(Sunny|Yes)= 3/9= 0.33\n","\n","- P(Yes)=9/14 = 0.64\n","\n","- P(Sunny)= 5/14 = 0.36\n","\n","- So, P(Yes|Sunny) = 0.33 * 0.64 / 0.36 =$\\approx$ 0.60\n","\n","- **Also we have to calculate**\n","\n","- P(No|Sunny)= P(Sunny|No)*P(No)/P(Sunny)\n","\n","- P(Sunny|NO)= 2/5=0.4\n","\n","- P(No)= 5/14=0.36\n","\n","- P(Sunny)= 5/14 = 0.36\n","\n","\n","- So P(No|Sunny)= 0.4*0.35/0.36 =$\\approx$0.40\n","\n","\n","\n","- **So as we can see from the above calculation that P(Yes|Sunny)>P(No|Sunny).Hence on a Sunny day, Player can play the game.**\n","\n","\n","- Similarly we can apply this process in multipal independent features at once   as follows.\n","\n","- Lets $y$ is dependent(target) variable and ($x_1$, $x_2$, $x_3$,....., $x_n$) are independent variables features.Then we can write above formula as follows also,\n","\n","$$P(y|x_1, x_2 ..... x_n) = \\frac{P(x_1, x_2 ... x_i ... x_n|y) \\ P(y)}{P(x_1, x_2 ... x_i ... x_n)}$$\n","\n","$$P(y|x_1, x_2 ..... x_n) = \\frac{[P(x_1|y)P(x_2|y)......P(x_n|y)] \\ P(y)}{P(x_1)P(x_2)..... P(x_n)}$$\n","\n","\n","\n"],"metadata":{"id":"UcBTlUC16vKb"}},{"cell_type":"markdown","source":["##**Common Probability Distributions**\n","\n","\n","##**(a)Gaussian Distribution**\n","\n","The most commonly used distribution over real numbers is the __normal distribution__, also known as the __Gaussian distribution__ is a **bell-shaped curve**.\n","\n","- **In Positive/Right Skew =** Mean > Median > Mode\n","- **In Negative/Left Skew=** Mode >Median >Mean\n","- **In Normal  Distribution=** Mean $\\approx$ Median$\\approx$Mode\n","\n","<figure align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1IPGTI806lUTb17Awyf8ytM2NCcVF8EXc\" height=\"300px\", width=\"500px\"> \n","</figure>\n","\n","- 99.73% of the data is within 3 standard deviations (σ) from the mean (μ).\n","\n","\n","$$ f(x) = \\frac{1}{ \\sigma \\sqrt{2\\pi}} e^ {\\frac{-(x- \\mu)^2 }{2 \\sigma^2}}. $$\n","\n","\n"," $\\mu$ = mean of x data feature.\n","\n"," $\\sigma$= standard deviation of x data feature.\n","  \n","$\\sigma^2$ = variance of  x data feature.\n","\n","\n","It is defined for all real values $x$, from $-\\infty$ to $\\infty$.\n","\n","\n","\n","The two parameters $\\mu \\in \\mathbb{R}$ and $\\sigma \\in (0, \\infty)$ control the normal distribution. The parameter $\\mu$ gives the coordinate of the central peak. This is also the mean of the distribution: $\\mathbb{E}[\\mathrm{x}] = \\mu$. The standard deviation of the distribution is given by $\\sigma$, and the variance by $\\sigma^2$."],"metadata":{"id":"H6vba4giCNtS"}},{"cell_type":"markdown","source":["#**Attribute Selection Measure for Splitting Decision Tree**\n","- In Decision tree we need to select the best attributes for the root node and for sub-nodes.**There is a technique which is called as Attribute selection measure(ASM).**\n","- In above figure **\"Salary is between\",\"Office near to home\",\"Provide cab facility\" are attributes.**\n","\n","- There are two popular techniques for ASM, which are **Information Gain and Gini Index.**\n","\n","\n","\n","#**Attribute Selection Measure for Splitting Tree**\n","- In Decision tree we need to select the best attributes for the root node and for sub-nodes.**There is a technique which is called as Attribute selection measure(ASM).**\n","- In above figure **\"Salary is between\",\"Office near to home\",\"Provide cab facility\" are attributes.**\n","\n","- There are two popular techniques for ASM, which are **Information Gain and Gini Index.**\n","\n","##**(a)Information Gain**\n","\n","- Information gain is the measurement of changes in entropy after the segmentation of a dataset based on an attribute.\n","\n","- A decision tree algorithm always tries to maximize the value of information gain, and  **Independent variable(feature) or node or attribute or having the highest information gain is split first.**\n","\n","- The **Independent variable(feature) having  higher  value of information gain work as a root node and so on.**\n","- It can be calculated using the below formula:\n","\n"," $$Information Gain= Entropy(TargetFeature)- [(Weighted Avg) *Entropy(IndependentFeature)]$$\n","\n","\n","- Entropy specifies randomness in data. Entropy can be calculated as:\n","\n","$$Entropy(TargetFeature)= -P(yes)log_2 P(yes)- P(no) log_2 P(no)$$\n","$$Entropy(IndependentFeature)= -P(yes)log_2 P(yes)- P(no) log_2 P(no)$$\n","\n","- Where,\n"," P(yes or 1)= probability of yes, P(no or 0)= probability of no.\n","- Entropy give more balanced tree so we used Entropy.\n","\n","\n","##**(b)Gini index**\n","- Gini index is a measure of impurity or purity used while creating a decision tree in the CART(Classification and Regression Tree) algorithm.\n","- An attribute(feature) with the low Gini index should be preferred as compared to the high Gini index **i.e smaller gini index chosen as the splitting point.**\n","\n","- Mathmatically Gini index can be calculated as follows:\n","\\begin{equation}\n","    \\text{Gini} = 1 - \\sum_{i=1}^{n} (P_{yes}^2+P_{No}^2)\n","\\end{equation}\n","\\begin{equation}\n","    \\text{Gini} = 1 - \\sum_{i=1}^{n} P_{i}^2\n","\\end{equation}\n","- Where,P(yes or 1)= probability of yes in Independent variable(feature), P(no or 0)= probability of no Independent variable(feature) and so on.\n","- Gini  is computationally fast  so we  used Gini Index."],"metadata":{"id":"zW9mtlgyqNeZ"}},{"cell_type":"markdown","source":["#**Statistics for Machine Learning**"],"metadata":{"id":"i1T6jjjVuqgl"}},{"cell_type":"markdown","source":["# Mean\n","\n","## Defining The Term\n","\n","* The **mean** is the *arithmetic average of a group of observations* (or numbers).\n","\n","* It is computed by summing all the observations and dividing by the total number of observations.\n","\n","* The population mean is represented by the Greek letter mu ($\\mu$), and the sample mean is represented by $\\overline{x}$.\n","\n","* The formulae for computing the population mean and the sample mean are given below:\n","    \n","    * Population Mean: $\\mu = \\frac{\\Sigma x}{N} = \\frac{x_{1} + x_{2} + x_{3} + .. + x_{N}}{N}$\n","    \n","    * Sample Mean: $\\overline{x} = \\frac{\\Sigma x}{n} = \\frac{x_{1} + x_{2} + x_{3} + .. + x_{n}}{n}$\n","    \n","* Let's break down the formulae:\n","    \n","    * The capital Greek letter *sigma*($\\Sigma$) is commonly used in mathematics to represent a summation of all the numbers in a grouping. \n","\n","    * *N* is the number of observations in the population, and *n* is the number of observation in the sample.\n","\n","## Example\n","\n","**Example 01:** The number of U.S. cars in service by top car rental companies in a recent year according to Auto Rental News follows.\n","\n","|Company|Number of Cars in Service|\n","|:-|-:|\n","|Enterprise|643,000|\n","|Hertz|327,000|\n","|National/Alamo|233,000|\n","|Avis|204,000|\n","|Dollar/Thrifty|167,000|\n","|Budget|144,000|\n","|Advantage|20,000|\n","|U-Save|12,000|\n","|Payless|10,000|\n","|ACE|9,000|\n","|Fox|9,000|\n","|Rent-A-Wreck|7,000|\n","|Triangle|6,000|\n","\n","Compute the mean.\n","\n","* **Solution:**\n","\n","Here we have a total of 12 observations, so *N=13*. \n","\n","$\\mu = $ $\\frac{643000 + 327000 + 233000 + 204000 + 167000 + 144000 + 20000 + 12000 + 10000 + 9000 + 9000 + 7000 + 6000}{13}$ \n","\n","$\\mu = $ $\\frac{1791000}{13}$ = $137769.23$\n","\n"],"metadata":{"id":"Suw-8bwS0vuj"}},{"cell_type":"markdown","source":["# Median\n","\n","## Defining The Term\n","\n","* The **median** is the *middle value* in an *ordered array of numbers*. It is the value that *splits the dataset in half*.\n","\n","* For an array with an *odd number* of terms, the median is the *middle number*. \n","\n","* For an array with an *even number* of terms, the median is the *average/mean of the two middle numbers*.\n","\n","* One needs to perform the followings steps to determine the median of a dataset:\n","    \n","    * Arrange the observations in an ordered data array.\n","    \n","    * For an odd number of terms, find the middle term of the ordered array. It is the median.\n","    \n","    * For an even number of terms, find the average of the middle two terms. This average/mean is the median.\n","    \n","* Another way is to use the following formula:\n","    \n","    * $Median = \\frac{n + 1}{2}^{th} term$\n","    \n","    * Here, n is the total number of observation in the dataset.\n","\n","## Example\n","\n","**Example 02:** Determine the median of the following dataset: 15, 11, 14, 3, 21, 17, 22, 16, 19, 16, 5, 7, 19, 8, 9, 20, 4.\n","\n","**Solution:** \n","\n","⟿ **Using The First Approach:**\n","\n","&emsp;&emsp; Arrange the numbers in an ordered array: 3, 4, 5, 7, 8, 9, 11, 14, 15, 16, 16, 17, 19, 19, 20, 21, 22\n","\n","&emsp;&emsp; Since, the array contains 17 terms (an odd number of terms), the median is the middle value. i.e. **15**.\n","\n","⟿ **Using formula:**\n","\n","&emsp;&emsp; Here n = 17, so $Median = \\frac{17 + 1}{2} = \\frac{18}{2} = 9^{th} term = 15$\n","\n","**Example 03:** Determine the median of the following dataset: 15, 11, 14, 3, 21, 17, 16, 19, 16, 5, 7, 19, 8, 9, 20, 4.\n","\n","**Solution:**\n","\n","⟿ **Using The First Approach:**\n","\n","&emsp;&emsp; Arrange the numbers in an ordered array: 3, 4, 5, 7, 8, 9, 11, 14, 15, 16, 16, 17, 19, 19, 20, 21\n","\n","&emsp;&emsp; Since, the array contains 16 terms (an even number of terms), the median is the average/mean of the two middle values. i.e. *14 and 15*.\n","\n","&emsp;&emsp; $Median = \\frac{14 + 15}{2} = 14.5$\n","\n","⟿ **Using formula:**\n","\n","&emsp;&emsp; Here n = 16, so $Median = \\frac{16 + 1}{2} = \\frac{17}{2} = 8.5^{th} term$\n","\n","&emsp;&emsp; *8.5<sup>th</sup>* term means the median is located halfway between *8<sup>th</sup>* and *9<sup>th</sup>* term or the average/mean of *14 and 15* which is **14.5**.\n"],"metadata":{"id":"zutiInmY1KXY"}},{"cell_type":"markdown","source":["# Mode\n","\n","## Defining The Term\n","\n","* The **mode** is the most frequently occurring value in a set of data. \n","\n","* On a bar chart, the mode is the highest bar. If no value repeats, the data do not have a mode.\n","\n","* A dataset can also have *more than one mode*.\n","\n","* We can use mode for qualitative as well as quantitative data. On the other hand, mean/median can only be used for quantitative data.\n","\n","* The mode has its limitations too. In some datasets, the mode may not reflect the center of the dataset very well. The presence of more than one mode can limit the ability of the mode in describing the center. Mode should only be used when we want to find the frequently occuring value in our dataset.\n","\n","## Example\n","\n","**Example 04:** Compute the mode of the following dataset: 1, 2, 2, 3, 4, 4, 5, 5, 5.\n","\n","**Solution:** Since the value *5* occurs most frequently(3 times). So, *5* is the *mode* of the given distribution.\n"],"metadata":{"id":"i6EQssxe1aLX"}},{"cell_type":"markdown","source":["##**Variance and Standard Deviation**\n","- In statistics, the variance is a measure of how far **individual (numeric) values in a dataset are from the mean or average value**.\n","\n","\n"," \\begin{align}\n","        \\sigma^2 = \\sum_{i=1}^n \\frac{(x_i -\\bar{x})}\n","        {(n-1)}\n","    \\end{align}\n","\n","- Standard deviation is a statistical measurement that looks at how far a **group of numbers is from the mean**.\n","\n","- Standard Deviation(S.D) =$\\sqrt{variance}$\n","\n"],"metadata":{"id":"uUeLrFXy6XSC"}},{"cell_type":"markdown","source":["##**Covariance**\n","\n","- In mathematics and statistics, covariance is a measure of the relationship between two random variables.\n","- Positive covariance: Indicates that two variables tend to move in the same direction.\n","- Negative covariance: Reveals that two variables tend to move in inverse directions.\n","\n","- Where, Covariance of each data point is calculated as follows;\n","\n"," \\begin{align}\n","        \\ cov(x,y) = \\sum_{i=1}^n \\frac{(x_i -\\bar{x}) (y_i -\\bar{y})}\n","        {(n-1)}\n","    \\end{align}"],"metadata":{"id":"Z7KyzkLf4m8q"}},{"cell_type":"markdown","source":["# **Correlation matrix:**\n"," Find the correlation between all independent variables of the dataset.Statisticians and data analysts measure correlation of two numerical variables to find an insight about their relationships.\n","\n","Pandas dataframe's method **.corr()** can be used to compute the correlation matrix.\n","\n","The correlation coefficient is a value that indicates the strength of the relationship between variables. The coefficient can take any values from -1 to 1. The interpretations of the values are:\n","\n","- **-1: Perfect negative correlation.** The variables tend to move in opposite directions (i.e., when one variable increases, the other variable decreases).\n","- **0: No correlation.** The variables do not have a relationship with each other.\n","- **1: Perfect positive correlation.** The variables tend to move in the same direction (i.e., when one variable increases, the other variable also increases).\n","\n","###**Formula:**\n","The correlation coefficient that indicates the strength of the relationship between two variables can be found using the following formula:\n","\n","correlation coefficient($r_{xy}$) =$\\frac{∑(x_i -\\bar{x})\t(y_i-\\bar{y})}{\\sqrt∑{(x_i-\\bar{x})^2\\sqrt ∑(y_i-\\bar{y})^2}}$\n","\n","\n","Where:\n","\n","- $r_{xy}$ – the correlation coefficient of the linear relationship between the variables x and y.\n","- $x_i$ – the values of the x-variable in a sample.\n","- $\\bar{x}$ – the mean of the values of the x-variable.\n","- $y_{i}$ – the values of the y-variable in a sample.\n","- $\\bar{y}$ – the mean of the values of the y-variable.\n","\n","\n","**In order to calculate the correlation coefficient using the formula above, you must undertake the following steps:**\n","\n","**step1:**Obtain a data sample with the values of x-variable and y-variable.\n","\n","**step2:**Calculate the means (averages) x̅ for the x-variable and ȳ for the y-variable.\n","\n","**step3:**For the x-variable, subtract the mean from each value of the x-variable (let’s call this new variable “a”). Do the same for the y-variable (let’s call this variable “b”).\n","\n","**step4:**Multiply each a-value by the corresponding b-value and find the sum of these multiplications (the final value is the numerator in the formula).\n","\n","**step5:**Square each a-value and b-value, calculate the sum of the result\n","\n","**step6:**Find the square root of the value obtained in the previous step (this is the denominator in the formula).\n","\n","**step7:**Divide the value obtained in step 4 by the value obtained in step 6.Finally we obtain the required correlation coefficient.\n","\n","\n","\n","##**Chi-square Test:**\n","- Chi-square test is a technique to determine the relationship between the **categorical variables.**\n","- The chi-square value is calculated between each feature and the target variable, and the desired number of features with the best chi-square value is selected.\n","- Mathmatically Chi-square value as:\n","$$\\large{\\chi ^2 = \\Sigma\\frac{(O_i - E_i)^2}{E_i}}$$\n","\n","\n","- Where, $\\chi ^2$=chi squared value , $O_i$=observed value are orginal dataset values, $E_i$=\texpected value = (total  sum  value of particular row  *total sum value of particular column)/Total sum value of whole dataset.\n","\n","- Higher the Chi-Square value of feature is more dependent on the response(target) and it can be selected for model training.\n","\n","\n"],"metadata":{"id":"-BQ9W5ymxL-_"}},{"cell_type":"markdown","source":["# **Identification of Outliers:**\n","Outliers are the values that look different from the other values in the data.Outliers can drastically change the results of the data analysis and statistical modeling.\n","###**Interquartile range(IQR) method for outlier detection:**\n","Quartile is a type of quantile which divides the number of data points into four parts.\n","\n","The first quartile (Q1) is defined as the middle number between the smallest number (minimum) and the median of the data set.It is also known as the lower or 25th empirical quartile, as 25% of the data is below this point.\n","\n","Let’s say that we have a data set with N data points then,\n","- Lower/First Quartile (Q1) = (N+1) * 1 / 4\n","\n","The second quartile (Q2) is also called median ; thus 50% of the data lies below this point.The median is the second quartile.\n","- Second Quartile/Median (Q2) = (N+1) * 1/2 \n","\n","\n","The third quartile (Q3) is the middle value between the median and the highest value (maximum) of the data set. It is known as the upper or 75th empirical quartile, as 75% of the data lies below this point.\n","- Upper/Third Quartile (Q3 )= (N+1) * 3 / 4\n","\n","Now we can fine the requires IQR\n","\n","- Inter Quartile Range (IQR) = 3rd Quartile - 1st Quartile\n","- Upper limit = 3rd Quartile + IQR * 1.5\n","- Lower limit = 1st Quartile - IQR * 1.5\n","- Any data above upper limit and below lower limit are called as outlier.\n","\n","\n","\n","##**Z- Score Method for Outlier Detection:**\n","- Z score is an important concept in statistics. Z score is also called standard score. \n","- This score helps to understand if a data value is greater or smaller than mean and how far away it is from the mean.\n","-  More specifically, Z score tells **how many standard deviations away a data point is from the mean.**\n","\n","- Z score = (x -mean) / std. deviation\n","\n","-  $ Z = \\frac{x - \\mu }{\\sigma} $\n","-  A standard cut-off value for finding outliers are Z-scores of **+/-3.**\n"],"metadata":{"id":"l9aTPLzZxdPR"}},{"cell_type":"markdown","source":["#**Evaluating the Performance of a Classification Model**\n","\n","##**Confusion Matrix:**\n","- The matrix is divided into two dimensions, that are predicted values and actual values.\n","<figure align=\"center\">\n","<img src=\"https://drive.google.com/uc?id=1EA0q7Kj0-9UQQj08k13F6RSkJWh3wrI4\" height=\"200px\", width=\"300px\"> \n","</figure>\n","\n","- The above matrix has the following cases:\n","\n","- **TP (True Positive):** Model has positive prediction $(\\hat{y} = 1)$ and actual value was also positive $(y = 1)$.\n","\n","- **TN (True Negative):**Model has negative prediction $(\\hat{y} = 0)$ and actual value was also negative $(y = 0)$.\n","\n","- **FP (False Positive)(Type I Error):** Model has positive prediction $(\\hat{y} = 1)$ but actual value was negative $(y = 0)$.\n","\n","- **FN (False Negative)(Type II Error):**Model has negative prediction $(\\hat{y} = 0)$ but actual value was positive $(y = 1)$.\n","\n","\n","\n","\n","\n","- We can used **from sklearn.metrics import plot_confusion_matrix** to compute the confusion matrix.\n","- With the help of confusion matrix we can evaluate the performance of a classification model as follows:\n","\n","##**(a)Accuracy:**\n","- It can be calculated as the ratio of the number of correct predictions made by the classifier to all number of predictions made by the classifiers. The formula is given below:\n","\n","$$\\text{Accuracy} = \\frac{\\text{Number of right predictions}}{\\text{Total number of predictions}}= \\frac{TP+TN}{TP+TN+FP+FN}$$\n","\n","- We can use **from sklearn.metrics import accuracy_score** to calculate accuracy score.\n","\n","- Accuracy may not be a good measure if the dataset is not balanced i.e Imbalance ratio between two or more classes.For Example 1000 message contain 990 not fraud and only 10 message are fraud.To solve this problem we apply the following methods.\n","\n","##**(b)Precision:**\n","- It can be calculated as the ratio of the number of True Positive made by the classifier to total number of positives prediction made by the classifiers. The formula is given below:\n","\n","$$\\text{Precision} = \\frac{\\text{True Positive}}{\\text{Total number of positives predicted}} = \\frac{TP}{TP+FP}$$\n"," \n","- We can use **from sklearn.metrics import precision_score** to calculate precision score.\n","\n","-  We apply Precision modul when the Type I Error(FP) is more dangerous.\n","\n","\n","##**(c)Recall:**\n","-  It is defined as the out of total positive classes, how our model predicted correctly. The recall must be as high as possible.\n","\n","$$\\text{Recall} =  \\frac{\\text{True Positive}}{\\text{Total number of actual positives}} = \\frac{TP}{TP + FN}$$\n","\n","- We can use **from sklearn.metrics import recall_score** to calculate recall score.\n","\n","- We apply Recall modul  when the Type II Error(FN) is more dangerous.\n","\n","##**(d)F1 Score:**\n","-  If two models have low precision and high recall or vice versa, it is difficult to compare these models. So, for this purpose, we can use F1-score. This score helps us to evaluate the recall and precision at the same time. The F-score is maximum if the recall is equal to the precision. It can be calculated using the below formula:\n","$$\\text{F1 score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision}  + \\text{Recall}}$$ \n","\n","- We can use **from sklearn.metrics import f1_score** to calculate F1-score.\n","\n","- We apply F1 Score module when it is difficult to find whether Type I error is more dangerous or Type II error more dangerous."],"metadata":{"id":"xY3KwLXCywty"}}]}